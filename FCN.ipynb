{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Fully Convolutional Network for Brain Tumor Segmentation\n",
    "\n",
    "### In this notebook we will train a fully convolutional network to perform segmentation on the BRATS dataset. We will use Tensorflow to construct the network and optimize its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoscrolling long output is disabled\n"
     ]
    }
   ],
   "source": [
    "import SimpleITK as sitk\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 24})\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "disable_js = \"\"\"\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def disable_scroll():\n",
    "    display(Javascript(disable_js))\n",
    "    print (\"autoscrolling long output is disabled\")\n",
    "    \n",
    "disable_scroll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the processed data\n",
    "\n",
    "#### Here we get the processed data using H5Py. H5Py allows us to access the data like we would with a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "X_p_train shape = (14622, 128, 128)\n",
      " Y_p_train shape = (14622, 128, 128)\n",
      " X_n_train shape = (16378, 128, 128)\n",
      " Y_n_train shape = (16378, 128, 128)\n",
      "X_p_test shape = (1474, 128, 128)\n",
      " Y_p_test shape = (1474, 128, 128)\n",
      " X_n_test shape = (1626, 128, 128)\n",
      " Y_n_test shape = (1626, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "data = h5py.File(data_path+'brats_train.h5','r')\n",
    "data_test = h5py.File(data_path+'brats_test.h5','r')\n",
    "\n",
    "\n",
    "X_p_train = data['X_positive']\n",
    "Y_p_train = data['Y_positive']\n",
    "X_n_train = data['X_negative']\n",
    "Y_n_train = data['Y_negative']\n",
    "print (np.amax(Y_p_train))\n",
    "print (\"X_p_train shape = {}\\n Y_p_train shape = {}\\n X_n_train shape = {}\\n Y_n_train shape = {}\".\n",
    "format(X_p_train.shape, Y_p_train.shape, X_n_train.shape, Y_n_train.shape))\n",
    "\n",
    "X_p_test = data_test['X_positive']\n",
    "Y_p_test = data_test['Y_positive']\n",
    "X_n_test = data_test['X_negative']\n",
    "Y_n_test = data_test['Y_negative']\n",
    "\n",
    "print (\"X_p_test shape = {}\\n Y_p_test shape = {}\\n X_n_test shape = {}\\n Y_n_test shape = {}\".\n",
    "format(X_p_test.shape, Y_p_test.shape, X_n_test.shape, Y_n_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow functions\n",
    "\n",
    "#### In this section we define some convenience functions to build our neural network.\n",
    "\n",
    "#### For this example we will train a convolutional network and use the leaky relu activation function. To make it easier to build our network we define a function that constructs a convolution layer `conv2D` and a function that applies many convolutional layers in sequence `conv_block`\n",
    "\n",
    "#### For training we need to sample batches of data from our dataset, so we also define a function `get_batch` which randomly selects positive and negative examples from our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x):\n",
    "    return tf.maximum(0.2*x,x)\n",
    "\n",
    "def conv2D(x, dims=[3, 3], filters=32, strides=[1, 1],\n",
    "           std=1e-3, padding='SAME', activation=tf.identity, scope='conv2d'):\n",
    "  \"\"\"\n",
    "  args:\n",
    "      x, (tf tensor), tensor with shape (batch,width,height,channels)\n",
    "      dims, (list), size of convolution filters\n",
    "      filters, (int), number of filters used\n",
    "      strides, (list), number of steps convolutions slide\n",
    "      std, (float/string), std of weight initialization, 'xavier' for xavier\n",
    "          initialization\n",
    "      padding, (string), 'SAME' or 'VALID' determines if input should be padded\n",
    "          to keep output dimensions the same or not\n",
    "      activation, (tf function), tensorflow activation function, e.g. tf.nn.relu\n",
    "      scope, (string), scope under which to store variables\n",
    "  returns:\n",
    "      a, (tf tensor), the output of the convolution layer, has size\n",
    "          (batch, new_width , new_height , filters)\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope):\n",
    "    s = x.get_shape().as_list()\n",
    "\n",
    "    shape = dims + [s[3], filters]\n",
    "\n",
    "    if std == 'xavier':\n",
    "      std = np.sqrt(2.0 / (s[1] * s[2] * s[3]))\n",
    "\n",
    "    W = tf.Variable(tf.random_normal(shape=shape, stddev=std), name='W')\n",
    "\n",
    "    b = tf.Variable(tf.ones([filters]) * std, name='b')\n",
    "\n",
    "    o = tf.nn.convolution(x, W, padding, strides=strides)\n",
    "\n",
    "    o = o + b\n",
    "\n",
    "    a = activation(o)\n",
    "\n",
    "    return a\n",
    "\n",
    "def conv_block(x,dims=[5,5],filters=16,num_layers=5,activation=tf.identity,scope='conv_block'):\n",
    "    o = x\n",
    "    with tf.variable_scope(scope):\n",
    "        for i in range(1,num_layers):\n",
    "            scope = 'conv_{}'.format(i)\n",
    "            o = conv2D(o,dims=dims,filters=filters,std=std,activation=act, scope=scope)\n",
    "    return o\n",
    "\n",
    "def get_batch(Xp,Yp, Xn, Yn, n=32):\n",
    "    Np = Xp.shape[0]\n",
    "    Nn = Xn.shape[0]\n",
    "    \n",
    "    inds_p = sorted(np.random.choice(range(Np),size=n, replace=False))\n",
    "    inds_n = sorted(np.random.choice(range(Nn),size=n, replace=False))\n",
    "    \n",
    "    xp = Xp[inds_p,:,:]\n",
    "    yp = Yp[inds_p,:,:]\n",
    "    \n",
    "    xn = Xn[inds_n]\n",
    "    yn = Yn[inds_n]\n",
    "    \n",
    "    x = np.concatenate((xp,xn))\n",
    "    y = np.concatenate((yp,yn))\n",
    "    \n",
    "    x = x[:,:,:,np.newaxis].astype(np.float32)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network parameters\n",
    "\n",
    "#### Specifying network parameters is where a lot of the black magic of Deep Learning happens. For this example we keep things simple by using 5 layers, with each layer having filter dimensions of 7 x 7. For training we try 10,000 iterations with a learning rate of 1e-3. A lot of these parameters are determined by trying out different values and seeing what works.\n",
    "\n",
    "#### Since each labeled image has few labeled pixels, we will use weights in our loss function to increase their importance. The more infrequent labels receive higher weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_steps = 10000\n",
    "Nbatch = 16\n",
    "print_step = 200\n",
    "\n",
    "W = X_p_train.shape[1]\n",
    "H = W\n",
    "C = 1\n",
    "\n",
    "num_layers = 5\n",
    "filters = 32\n",
    "dims = [7,7]\n",
    "strides = [1,1]\n",
    "\n",
    "act = leaky_relu\n",
    "\n",
    "std=1e-2\n",
    "num_classes=5\n",
    "learning_rate=1e-3\n",
    "\n",
    "class_weights= np.asarray([1.0, 60.0, 20.0, 80.0, 80.0])\n",
    "# class_weights = class_weights/np.sum(class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "Here we construct the tensorflow graph for our neural network.\n",
       "\n",
       "The first step is to construct placeholder variables for the input. The images need a placeholder variable\n",
       "with size $W x H x C$, and the segmentations need a placeholder with dimensions $W x H$.\n",
       "\n",
       "Ater that we apply a series of convolutional layers with the specified amount of filters, followed by a final\n",
       "convolutional layer with the same number of filters as the number of output classes for each pixel.\n",
       "\n",
       "For the loss function we use a weighted cross entropy loss function\n",
       "$L = \\sum_{i}\\beta_i \\sum_{jk}-y_{jki}\\log(\\hat{y}_{jki})$ where $\\beta_i$ is the weight for each class\n",
       "\n",
       "The weights help the network learn to indentify labels which are rare."
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\n",
    "Here we construct the tensorflow graph for our neural network.\n",
    "\n",
    "The first step is to construct placeholder variables for the input. The images need a placeholder variable\n",
    "with size $W x H x C$, and the segmentations need a placeholder with dimensions $W x H$.\n",
    "\n",
    "Ater that we apply a series of convolutional layers with the specified amount of filters, followed by a final\n",
    "convolutional layer with the same number of filters as the number of output classes for each pixel.\n",
    "\n",
    "For the loss function we use a weighted cross entropy loss function\n",
    "$L = \\sum_{i}\\beta_i \\sum_{jk}-y_{jki}\\log(\\hat{y}_{jki})$ where $\\beta_i$ is the weight for each class\n",
    "\n",
    "The weights help the network learn to indentify labels which are rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow graph construction\n",
    "\n",
    "#construct input place holders\n",
    "x = tf.placeholder(shape=[None,W,H,C],dtype=tf.float32)\n",
    "y = tf.placeholder(shape=[None,W,H],dtype=tf.int32)\n",
    "\n",
    "y_truth = tf.one_hot(y,depth=num_classes,axis=3)\n",
    "\n",
    "#####################################\n",
    "# Convolutional part\n",
    "#####################################\n",
    "o = conv_block(x,dims,filters,num_layers,act,scope=\"conv_block_1\")\n",
    "\n",
    "o = conv2D(o,dims=dims,filters=num_classes,strides=strides,std=std,\n",
    "           activation=tf.identity, scope='conv_final')\n",
    "\n",
    "yhat = tf.nn.softmax(o)\n",
    "\n",
    "l_temp = tf.reduce_mean(-y_truth*tf.log(yhat+1e-5),axis=[0,1,2])\n",
    "\n",
    "l_temp = l_temp*class_weights\n",
    "\n",
    "loss = tf.reduce_mean(l_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "#### Tensorflow provides optimizer objects we can use to optimizer our network. Here we use the AdamOptimizer and use it to minimize our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construct the optimizer and training operations\n",
    "opt = tf.train.AdamOptimizer(learning_rate)\n",
    "train = opt.minimize(loss)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construct the tensorflow session and initialize the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "#### In this section of the code we train the network and occasionally evalute its performance on the test set. In each training iteration a different batch of data is sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the train loop\n",
    "train_hist = []\n",
    "val_hist = []\n",
    "\n",
    "for i in range(train_steps):\n",
    "    xb,yb = get_batch(X_p_train,Y_p_train,X_p_train,Y_p_train,n=Nbatch)\n",
    "    l,_=sess.run([loss,train],{x:xb,y:yb})\n",
    "\n",
    "    if i%print_step == 0:\n",
    "        xb,yb = get_batch(X_p_test,Y_p_test,X_n_test,Y_n_test,n=Nbatch)\n",
    "        lval=sess.run(loss,{x:xb,y:yb})\n",
    "        print (\"iter: {} Train: {} Val: {}\".format(i,l,lval))\n",
    "        train_hist.append(l)\n",
    "        val_hist.append(lval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Curves\n",
    "\n",
    "#### After training we can look at the loss curves to see whether our network is overfitting, underfitting or performing well. Due to training on batches the loss function is erratic, in this case it would be advisable to train for more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_hist,color='r',linewidth=2,label='train')\n",
    "plt.plot(val_hist,color='g',linewidth=2,label='test')\n",
    "plt.xlabel('iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Predictions\n",
    "\n",
    "#### We can now evaluate the predicted segmentations our network makes by computing predictions on the test set and visualizing them. Our network does a reasonable job at identifying the general locations of the tumors and their severity which is not bad considering that we used a simple convolutional network. However, in some cases the network over segments or misses key parts of the tumor.\n",
    "\n",
    "#### To get better performance we could experiment with different neural networks, e.g. trying more layers, more parameters, different activation functions, experimenting with pooling layers etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "def implot(mp,ax,cmap='gray'):\n",
    "    im = ax.imshow(mp.astype(np.float32), cmap=cmap)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = plt.colorbar(im, cax=cax)\n",
    "\n",
    "    \n",
    "xb,yb = get_batch(X_p_test,Y_p_test,X_n_test,Y_n_test,n=Nbatch)\n",
    "\n",
    "yh = sess.run(yhat,{x:xb})\n",
    "\n",
    "ypred = np.argmax(yh,axis=3)\n",
    "\n",
    "for i in range(7):\n",
    "\n",
    "    plt.figure()\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True,figsize=(10,3))\n",
    "    implot(xb[i,:,:,0],ax1)\n",
    "    implot(yb[i,:,:],ax2,cmap='spectral')\n",
    "    implot(ypred[i,:,:],ax3,cmap='spectral')\n",
    "    plt.grid('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images_{}.pdf'.format(i),dpi=600)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
